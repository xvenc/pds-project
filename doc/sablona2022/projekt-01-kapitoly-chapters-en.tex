% This file should be replaced with your file with an thesis content.
%=========================================================================
% Authors: Michal Bidlo, Bohuslav Křena, Jaroslav Dytrych, Petr Veigend and Adam Herout 2019

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}

\chapter{Log file analysis introduction}

This chapter describes the problem of log file analysis and the motivation for this work. It's main goal is to provide a brief background to understand the problem, purpose and methods of log file 
analysis and anomaly detection. At the end of the chapter is brief overview of the content of other chapters.

Modern computer systems generate a large amount of log data. This data is used for monitoring, debugging, and security purposes. Today's systems also grow in complexity either by 
scaling out to distributed systems like Hadoop, Spark, Kubernetes, or by scaling up to high performance computing like Blue Gene. These systems support a large variety of online services and applications.
Therefore these systems tend to run 24/7 and are expected to be highly available and any downtime is considered as a loss of revenue. This makes it important to monitor these systems and detect any abnormal 
behaviour as soon as possible.

All these logs are the main data source for system anomaly detection. For traditional standalone systems, developers manually check system logs or write rules to detect anomalies 
based on their domain knowledge. They used regular expression mating combined with finding specific words in the log files (e.g., “fail”, “exception”). However, such anomaly detection that relies heavily on
manual inspection of logs is not scalable and is not suitable for modern systems. Modern systems generate a large amount of log data and it is not feasible to manually inspect all logs. Modern systems 
are generating around 120 to 200 milion lines of log entries per hour. So you can see the problem at that's just the tip of the iceberg in log file analysis. Another issue is that the log files aren't 
that much structured and are in a free text format. This makes it difficult to extract useful information from the log files.

But the log file analysis and processing can be crucial in many areas. For example, in the security area, log files are used to detect security breaches and attacks. 
In the monitoring area, log files are used to detect system failures and performance issues. In the debugging area, log files are used to detect software bugs and issues. %(https://www.researchgate.net/publication/335883136_Log_File_Analysis_as_a_Method_for_Automated_Measurement_of_Internet_Usage) 
So it is important to develop methods and tools to automatically process and analyze log files. 

The process of log file analysis for anomaly detection can be divided into 4 main steps.
\begin{enumerate}
    \item Collect log files from the system. The log files can be collected from various sources like system logs, application logs, network logs, etc. 
    \item Parse the log files. The log files are in a free text format and are not structured. So the first step is to parse the log files and extract useful information from the log files.
    \item Extract useful information from the log files. The log files contain a lot of information, but usually it's hard to retrive these infomations automaticaly from the unstructured text format. The next step is to extract these useful informations from the log files. These informations than can be used for anomaly detection.
    \item Detect anomalies in the log files. The last step is to use some methods to detect anomalies in the log files. Anomalies are the events that are different from the normal behaviour of the system. The goal is to detect these anomalies as soon as possible to prevent system failures and security breaches.
\end{enumerate}

There are various methods that can be used to detect anomalies from the log files. In this project I focused on the machine learning methods for anomaly detection in log files.
But there are also other methods for anomaly detection in log files (e.g. statistical methods). %https://www.researchgate.net/publication/325191790_Anomaly_Detection_Techniques

Based on the type of data and the machine learning techniques used, anomaly detection methods can be classified into two broad categories: supervised anomaly detection and unsupervised
anomaly detection. Supervised methods require labeled data, which means that the data is labeled as normal or abnormal. Unsupervised methods do not require labeled data. In this project I focused on 
the supervised anomaly detection methods.

In the next chapter I will describe the log dataset that I used for the experiments. In the third chapter I will focus on the data preprocessing and feature extraction and machine learning model selection.
In the fourth chapter I will describe the implementation of the the and conducted experiments and also the results of these experiments. The last chapter is conclusion of the work.

\chapter{Description and analysis of Log Dataset}

There are plenty of log datasets available for research purposes. As I mentioned before the log dataset are often in form of unstructured text data.
A log message records a specific event that occured in the system. Usually it has following set of fields: \textbf{timsstamp} (time when the event occurred), \textbf{log level} (the severity
of the event, e.g INFO, WARNING) and \textbf{message} that descibes the event in free text format. An example of log message is shown bellow.

\begin{center}

    \fbox{
        \begin{minipage}{0.8\linewidth} % Adjust the width as needed
            2023-03-02 20:25:56 INFO dfs.DataNode\$DataXceiver: \\Receiving block blk\_-1608999687919862906 src: 10.250.19.102:54106 dest: 10.250.19.102:50010
        \end{minipage}
      }

\end{center}

There are many publicly available datasets published by various organizations and authors. Popular datasets are for exapmle: \textbf{Loghub}, % add footnote
\textbf{Secrepo} or \textbf{Stratosphere labs}. For this project I used the log dataset provided by \textbf{Loghub}. This dataset is public and free to use. 
More specifically I used the HDFS v1 dataset about which I will provide more details in the next section.

Generally datasets can be divided into two groups: labelled and unlabelled datasets. The labelled datasets contain log messages that are labeled as normal or abnormal. The unlabelled datasets contain only log messages without any labels.
I focused on the labelled datasets, because I used supervised machine learning methods for anomaly detection, but more about this is chapter \ref{model}.

\section{HDFS log dataset} \label{hdfs}

The dataset I chose for this project is the HDFS v1 dataset from the Loghub. The collection of the individual log files is described in this paper: % add citation. 
This dataset contains log messages from the Hadoop Distributed File System (HDFS). The HDFS is a distributed 
system designed to run on a large cluster of commodity hardware. The Loghub provides 3 versions of the HDFS dataset: HDFS v1, HDFS v2 and HDFS v3. I chose the HDFS v1 dataset, because it's annotated with labels.

HDFS-v1 is generated in a 203-nodes HDFS using benchmark workloads and was manually labeled through handcrafted rules to differ between normal and abnormal events. The original dataset contains \texttt{11 175 629} log entries.
From this \texttt{16 838} are labelled as anomalies and the rest as normal. Example of the log message from the HDFS v1 dataset is shown bellow.

\begin{center}
    \fbox{
  \begin{minipage}{0.8\linewidth} % Adjust the width as needed
    081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/job/job.jar. blk\_16089996
  \end{minipage}
}
\end{center}

Where the fields are: \textbf{timestamp} (081109 203518),\textbf{PID} (35) \textbf{log level} (INFO), \textbf{component} (dfs.FSNamesystem) and \textbf{message} (BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job\_200811092030\_0001/job.jar. blk\_1608999687919862906).
Where very important role in this dataset plays the \texttt{message} field, because firstly it containts the info what happend, but mainly because it contains the block id, which is the unique identifier of the log message.
This \textbf{block id} is then used in the file with the labels.

Another important part of the dataset are the labels, which are used for the supervised machine learning methods. All the labels are in separate file. where each line contains previously mentioned block id and the label (normal or abnormal).
The entry in label file looks like this: \texttt{blk\_1608999687919862906, normal}. This means that the log message with the block id \texttt{blk\_1608999687919862906} is normal.

The last important part of the dataset is the template file. This file contains the templates for the log messages. The templates are used to match one line of log message with individual event templates. The event templates are in a form close to regular expressions.
The whole table with the templates is shown in the table \ref{tab:templates} in appendix \ref{appendix}. So these template events are then used as features describing the log entry. The features are then used as input for the machine learning model.
The template file was generated for the HDFS v1 dataset by the authors of the dataset. The template contains 30 unique events, but more details on this topic is in the next chapter.

\chapter{Modelling log events}
In this chapter I will firstly describe the process of data preprocessing, then I will focus on feature extraction and at the end will talk about model selection for the log file analysis. 
The main goal of this chapter is to provide a brief overview of the methods used in this project.


\section{Data preprocessing}
The data preprocessing is the first step in the log file analysis. The log files are in a free text format and are not structured. 
The log preprocessing is the process of converting the log files into a structured format that can be used for further analysis. It's very important part 
of the log file analysis, because the log files are usually in a free text format and are not structured, but most of the machine learning models require structured data as input.
So the first step is to use some methods to parse the log files and extract useful information from the log files. 

Typically, log parsing problem is clustering problem, where the goal is to cluster the log messages into groups based on their similarity. The log messages in the same group
should describe the same system event and the output of this clustering should be in some form of template file. This template file is then used to match the log messages to the templates. 
This can be done by number of existing log parsers. % logparser cite https://jiemingzhu.github.io/pub/slhe_issre2016.pdf
Thankfuly I didn't have to do the clustering to create the template file, because the authors of the HDFS v1 dataset already did it and they provided the template file. 

So the template file contains 30 unique \texttt{template events} (further will be refered only as events). These events consist of two parts: \texttt{constant} and \texttt{variable}. 
The constant part is the part of the event that is the same for all log messages that match the event. The variable part is the part of the event that can change in the log messages. 
As you can see on the example bellow. In the first box \ref{tab:full_part} is show a log entry we already saw in the previous chapter. 
In the second box \ref{tab:template_part} is shown the event template that matches the log entry. 
The constant part of the event is the same as the log entry, but the variable part is replaced by the \texttt{.+} which means that the variable part can be any string. So this log 
entry would be event number 22, or in short \textbf{E22}.

\begin{center}
    \label{tab:full_part}
    \fbox{
  \begin{minipage}{0.8\linewidth} % Adjust the width as needed
    081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/job/job.jar. blk\_16089996
  \end{minipage}
}
\end{center}

\begin{center}
    \label{tab:template_part}
    \fbox{
  \begin{minipage}{0.8\linewidth} 
   E22: .\*BLOCK\* NameSystem\.allocateBlock: .+ 
  \end{minipage}
}
\end{center}

So to correctly preprocess the log files, the log messages are matched to the event templates.
So each line is split into 5 parts: \texttt{timestamp}, \texttt{PID}, \texttt{log level}, \texttt{component} and \texttt{message}. The \texttt{message} part is then matched to the event templates 
using regular expressions. If the message part matches the event template, the log message is assigned the event number. If the message part doesn't match any event template, the log message is assigned \textbf{E0}.

\section{Feature extraction}

This section describes the feature extraction process. The feature extraction is the process of extracting useful information from the log files, so it can be used as input for the machine learning model.
As I mentioned before log file are in a free text format and are not structured, which is not suitable for machine learning models, because they require structured data as input and 
best of all they require numerical data as input. So the goal of the feature extraction is to convert the log messages into numerical data that can be used as input for the machine learning model.
For this I used one of the approaches proposed in the paper \cite{loglizer}. This approach is called \textbf{session window}. The another two approaches are \textbf{sliding window} and \textbf{fixed window}, but I didn't use them in this project.

The session window approach is based on identifiers instead of the timestamp, which is used in the sliding window and fixed window approaches. 
The indentifiers are the \texttt{block IDs}. Therefore, we can group the log entries according to the indetifiers, where each session window has it's own unique identifier.
So if a new block ID is encountered, a new session window is created with coresponding event. It's obvious that the block ID can occure in multiple log entries, hence if the block ID is already in the session window, then the event is appended to the session window.

Then after parsing all the log entries a so called event count matrix \texttt{X} is created. For each block ID, the number of occurences of each event is counted to form the event count vector.
For example, if the event count vector is \texttt{[0, 0, 2, 3, 0, 0, 1]}, it means that the event 3 occured 2 times, the event 4 occured 3 times and the event 6 occured 1 time for the block ID.
So the event count matrix is a matrix where each row represents the event count vector for the block ID. The event count matrix is then used as input for the machine learning model.

\section{Model selection} \label{model}

The last part of this chapter is the model selection. As I mentioned before I used the supervised machine learning methods for anomaly detection in log files. Supervised learning can be defined 
as a machine learning task of creating a model from labeled training data that can make predictions on unseen data.
The prerequisite of supervised anomaly detection are labeled data, which means that the data is labeled as normal or anomaly. The more the labeled data the better the model can be trained.
For this project three machine learning methods were considered: \textbf{Logistic Regression}, \textbf{SVM} and \textbf{Random Forest}. And the model selected for the experiments was the Random Forest.
Mainly because the Random Forest is the most familiar to me, because I used it in the past in my bachelor's thesis. It's also robust to overfitting and can handle large datasets with higher dimensionality.
A simple example of the Random Forest model is shown in the figure \ref{fig:random_forest}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{obrazky-figures/random_forest.drawio.png}
    \caption{Simplified picture of how Random Forest can look like.}
    \label{fig:random_forest}
\end{figure}

The implementation of the Random Forest model was done using the \texttt{scikit-learn}\footnote[1]{\url{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html} (visited 29.3.2024)} library in Python. 
The \texttt{scikit-learn} library is a free software machine learning library for the Python programming language.
Firstly the basic model without any hyperparameter tuning was used. The hyperparameters are the parameters that are set before the learning process begins.
The list of all the hyperparameters before and after tuning can be found in the appendix \ref{appendix}. 
Then I tuned the hyperparameters using the \texttt{GridSearchCV}\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html} (visited 29.3.2024)} method from the \texttt{scikit-learn} library. The \texttt{GridSearchCV} method is used to tune the hyperparameters of the model by searching over a grid of hyperparameters.
I experimented only with a subset of the hyperparameters, because the tuning process is computationally expensive and time consuming. 
So the final model has following hyperparameters: 
TODO

\chapter{Tool implementation and experiments}

This chapter describes the implementation of the tool and the conducted experiments. 
The main goal of this chapter is to provide a brief overview of the implementation and the results of the experiments.
The implementation of the tool was done in Python using the \texttt{scikit-learn}\footnote{\url{https://scikit-learn.org/stable/} (visited 29.3.2024)}
library for the machine learning model and the \texttt{pandas}\footnote{\url{https://pandas.pydata.org/} (visited 29.3.2024)} library for data manipulation.
The whole list of required packages, process of installation and some examples can be found in the appendix \ref{appendix} or in the \texttt{Readme.md} file.

The script can be run from the command line and this is an example run of the script:
\begin{verbatim}
    python3 log-monitor.py --log_file logs/HDFS-test.log --template 
    logs/HDFS_templates.csv --labels logs/anomaly_label.csv
\end{verbatim}

The input parameters are the log file, the template file and the label file. The log file is the log file that contains the log messages. 
The template file is the file that contains the event templates and the label file is the file that contains the labels for the log entries.

\section{Implementation}

The implementation of the tool consists of several parts and it's divided into several files. 
The main file is the \texttt{log-monitor.py} file. This file is the main file that contains the main function and function to parse the command line arguments.
Next file is the \texttt{dataload.py} file. This file contains the class \textbf{DataLoader} that is used to load the log file, the template file and the label file. Another
purpose of this class is match the log entries with the event templates and labels.
Then there is the \texttt{preprocess.py} file. This file contains the class \textbf{Preprocess} that is used to preprocess the log entries and extract the features and create the 
event count matrix. 
The last file is the \texttt{model.py} file. This file contains the class \textbf{Model} that is used to train, tune and evaluate the machine learning model.
The whole process of the tool is shown in the figure \ref{fig:tool}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{obrazky-figures/tool.drawio.png}
    \caption{The process of the tool.}
    \label{fig:tool}
\end{figure}



\section{Experiments}

\chapter{Conclusion}

TODO - write a conclusion.

%=========================================================================

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}