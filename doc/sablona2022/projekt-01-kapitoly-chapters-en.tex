% This file should be replaced with your file with an thesis content.
%=========================================================================
% Authors: Michal Bidlo, Bohuslav Křena, Jaroslav Dytrych, Petr Veigend and Adam Herout 2019

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}

\chapter{Log file analysis introduction}

This chapter describes the problem of log file analysis and the motivation for this work. It's main goal is to provide a brief background to understand the problem, purpose and methods of log file 
analysis and anomaly detection. At the end of the chapter is brief overview of the content of other chapters.

Modern computer systems generate a large amount of log data. This data is used for monitoring, debugging, and security purposes. Today's systems also grow in complexity either by 
scaling out to distributed systems like Hadoop, Spark, Kubernetes, or by scaling up to high performance computing like Blue Gene. These systems support a large variety of online services and applications.
Therefore these systems tend to run 24/7 and are expected to be highly available and any downtime is considered as a loss of revenue. This makes it important to monitor these systems and detect any abnormal 
behaviour as soon as possible.

All these logs are the main data source for system anomaly detection. For traditional standalone systems, developers manually check system logs or write rules to detect anomalies 
based on their domain knowledge. They used regular expression mating combined with finding specific words in the log files (e.g., “fail”, “exception”). However, such anomaly detection that relies heavily on
manual inspection of logs is not scalable and is not suitable for modern systems. Modern systems generate a large amount of log data and it is not feasible to manually inspect all logs. Modern systems 
are generating around 120 to 200 milion lines of log entries per hour. So you can see the problem at that's just the tip of the iceberg in log file analysis. Another issue is that the log files aren't 
that much structured and are in a free text format. This makes it difficult to extract useful information from the log files.

But the log file analysis and processing can be crucial in many areas. For example, in the security area, log files are used to detect security breaches and attacks. 
In the monitoring area, log files are used to detect system failures and performance issues. In the debugging area, log files are used to detect software bugs and issues. %(https://www.researchgate.net/publication/335883136_Log_File_Analysis_as_a_Method_for_Automated_Measurement_of_Internet_Usage) 
So it is important to develop methods and tools to automatically process and analyze log files. 

The process of log file analysis for anomaly detection can be divided into 4 main steps.
\begin{enumerate}
    \item Collect log files from the system. The log files can be collected from various sources like system logs, application logs, network logs, etc. 
    \item Parse the log files. The log files are in a free text format and are not structured. So the first step is to parse the log files and extract useful information from the log files.
    \item Extract useful information from the log files. The log files contain a lot of information, but usually it's hard to retrive these infomations automaticaly from the unstructured text format. The next step is to extract these useful informations from the log files. These informations than can be used for anomaly detection.
    \item Detect anomalies in the log files. The last step is to use some methods to detect anomalies in the log files. Anomalies are the events that are different from the normal behaviour of the system. The goal is to detect these anomalies as soon as possible to prevent system failures and security breaches.
\end{enumerate}

There are various methods that can be used to detect anomalies from the log files. In this project I focused on the machine learning methods for anomaly detection in log files.
But there are also other methods for anomaly detection in log files (e.g. statistical methods). %https://www.researchgate.net/publication/325191790_Anomaly_Detection_Techniques

Based on the type of data and the machine learning techniques used, anomaly detection methods can be classified into two broad categories: supervised anomaly detection and unsupervised
anomaly detection. Supervised methods require labeled data, which means that the data is labeled as normal or abnormal. Unsupervised methods do not require labeled data. In this project I focused on 
the supervised anomaly detection methods.

In the next chapter I will describe the log dataset that I used for the experiments. In the third chapter I will focus on the data preprocessing and feature extraction and machine learning model selection.
In the fourth chapter I will describe the implementation of the the and conducted experiments and also the results of these experiments. The last chapter is conclusion of the work.

\chapter{Description and analysis of Log Dataset}

There are plenty of log datasets available for research purposes. As I mentioned before the log dataset are often in form of unstructured text data.
A log message records a specific event that occured in the system. Usually it has following set of fields: \textbf{timsstamp} (time when the event occurred), \textbf{log level} (the severity
of the event, e.g INFO, WARNING) and \textbf{message} that descibes the event in free text format. An example of log message is shown bellow.

\begin{center}

    \fbox{
        \begin{minipage}{0.8\linewidth} % Adjust the width as needed
            2023-03-02 20:25:56 INFO dfs.DataNode\$DataXceiver: \\Receiving block blk\_-1608999687919862906 src: 10.250.19.102:54106 dest: 10.250.19.102:50010
        \end{minipage}
      }

\end{center}

There are many publicly available datasets published by various organizations and authors. Popular datasets are for exapmle: \textbf{Loghub}, % add footnote
\textbf{Secrepo} or \textbf{Stratosphere labs}. For this project I used the log dataset provided by \textbf{Loghub}. This dataset is public and free to use. 
More specifically I used the HDFS v1 dataset about which I will provide more details in the next section.

Generally datasets can be divided into two groups: labelled and unlabelled datasets. The labelled datasets contain log messages that are labeled as normal or abnormal. The unlabelled datasets contain only log messages without any labels.
I focused on the labelled datasets, because I used supervised machine learning methods for anomaly detection, but more about this is chapter \ref{model}.

\section{HDFS log dataset} \label{hdfs}

The dataset I chose for this project is the HDFS v1 dataset from the Loghub. The collection of the individual log files is described in this paper: % add citation. 
This dataset contains log messages from the Hadoop Distributed File System (HDFS). The HDFS is a distributed 
system designed to run on a large cluster of commodity hardware. The Loghub provides 3 versions of the HDFS dataset: HDFS v1, HDFS v2 and HDFS v3. I chose the HDFS v1 dataset, because it's annotated with labels.

HDFS-v1 is generated in a 203-nodes HDFS using benchmark workloads and was manually labeled through handcrafted rules to differ between normal and abnormal events. The original dataset contains \texttt{11 175 629} log entries.
From this \texttt{16 838} are labelled as anomalies and the rest as normal. Example of the log message from the HDFS v1 dataset is shown bellow.

\begin{center}
    \fbox{
  \begin{minipage}{0.8\linewidth} % Adjust the width as needed
    081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/job/job.jar. blk\_16089996
  \end{minipage}
}
\end{center}

Where the fields are: \textbf{timestamp} (081109 203518),\textbf{PID} (35) \textbf{log level} (INFO), \textbf{component} (dfs.FSNamesystem) and \textbf{message} (BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job\_200811092030\_0001/job.jar. blk\_1608999687919862906).
Where very important role in this dataset plays the \texttt{message} field, because firstly it containts the info what happend, but mainly because it contains the block id, which is the unique identifier of the log message.
This \textbf{block id} is then used in the file with the labels.

Another important part of the dataset are the labels, which are used for the supervised machine learning methods. All the labels are in separate file. where each line contains previously mentioned block id and the label (normal or abnormal).
The entry in label file looks like this: \texttt{blk\_1608999687919862906, normal}. This means that the log message with the block id \texttt{blk\_1608999687919862906} is normal.

The last important part of the dataset is the template file. This file contains the templates for the log messages. The templates are used to match one line of log message with individual event templates. The event templates are in a form close to regular expressions.
The whole table with the templates is shown in the table \ref{tab:templates} in appendix \ref{appendix}. So these template events are then used as features describing the log entry. The features are then used as input for the machine learning model.
The template file was generated for the HDFS v1 dataset by the authors of the dataset. The template contains 30 unique events, but more details on this topic is in the next chapter.

\chapter{Modelling log events}
In this chapter I will firstly describe the process of data preprocessing, then I will focus on feature extraction and at the end will talk about model selection for the log file analysis. 
The main goal of this chapter is to provide a brief overview of the methods used in this project.


\section{Data preprocessing}
The data preprocessing is the first step in the log file analysis. The log files are in a free text format and are not structured. 
The log preprocessing is the process of converting the log files into a structured format that can be used for further analysis. It's very important part 
of the log file analysis, because the log files are usually in a free text format and are not structured, but most of the machine learning models require structured data as input.
So the first step is to use some methods to parse the log files and extract useful information from the log files. 

Typically, log parsing problem is clustering problem, where the goal is to cluster the log messages into groups based on their similarity. The log messages in the same group
should describe the same system event and the output of this clustering should be in some form of template file. This template file is then used to match the log messages to the templates. 
This can be done by number of existing log parsers. % cite here the log parsers from here https://arxiv.org/pdf/2008.06448.pdf page 5
Thankfuly I didn't have to do the clustering to create the template file, because the authors of the HDFS v1 dataset already did it and they provided the template file. 

So the template file contains 30 unique template events (further will be refered only as events).



\section{Feature extraction}

\section{Model selection} \label{model}

\chapter{Tool implementation and experiments}

\section{Implementation}

\section{Experiments}

\chapter{Conclusion}

TODO - write a conclusion.

%=========================================================================

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}